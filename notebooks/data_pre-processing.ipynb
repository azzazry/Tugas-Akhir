{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df0ac139",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec748a9",
   "metadata": {},
   "source": [
    "`step 0`: Import semua library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b530ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import semua library yang dibutuhkan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import pickle\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7318bf1",
   "metadata": {},
   "source": [
    "## Merge LDAP folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2431add5",
   "metadata": {},
   "source": [
    "`step 1`: Gabungkan dulu folder LDAP menjadi users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c763c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load semua file .csv dalam folder LDAP\n",
    "# ldap_files = glob.glob('../data/r6.2/LDAP/*.csv')\n",
    "# print(f\"Found {len(ldap_files)} LDAP files:\")\n",
    "\n",
    "# # Membaca dan menggabungkan file .csv\n",
    "# all_data = []\n",
    "# for file in ldap_files:\n",
    "#     print(f'Loading: {file}')\n",
    "#     df = pd.read_csv(file)\n",
    "#     print(f\"  - {len(df)} rows, {df['user_id'].nunique()} unique users\")\n",
    "#     all_data.append(df)\n",
    "\n",
    "# # Gabung semua data\n",
    "# combined_df = pd.concat(all_data, ignore_index=True)\n",
    "# print(f\"\\nBefore cleanup:\")\n",
    "# print(f\"Total rows: {len(combined_df)}\")\n",
    "# print(f\"Unique users: {combined_df['user_id'].nunique()}\")\n",
    "\n",
    "# # Remove duplicates berdasarkan user_id\n",
    "# users_df = combined_df.drop_duplicates(subset=['user_id'], keep='first')\n",
    "# print(f\"\\nAfter removing duplicates:\")\n",
    "# print(f\"Total rows: {len(users_df)}\")\n",
    "# print(f\"Unique users: {users_df['user_id'].nunique()}\")\n",
    "\n",
    "# # Save hasil\n",
    "# users_df.to_csv('../data/r6.2/users.csv', index=False)\n",
    "# print(f\"\\nSaved to: users.csv\")\n",
    "# print(f\"Shape: {users_df.shape}\")\n",
    "# users_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca32afe",
   "metadata": {},
   "source": [
    "## Load Data dan Filter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b51802",
   "metadata": {},
   "source": [
    "`step 2`: Load dataset dari file CSV </br>\n",
    "- Pada tahapan ini, ada skenario unik dimana file http itu tidak dapat di load, alih-alih menggunakan pandas peneliti akan menggunakan dask untuk membaca file dengan data besar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a4c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "_raw_path = '../data/r6.2/'\n",
    "_label_path = '../data/labels/'\n",
    "\n",
    "# Load raw dataset\n",
    "logon_df = pd.read_csv(f'{_raw_path}logon.csv')\n",
    "users_df = pd.read_csv(f'{_raw_path}filtered_users.csv')\n",
    "file_df = pd.read_csv(f'{_raw_path}file.csv')\n",
    "device_df = pd.read_csv(f'{_raw_path}device.csv')\n",
    "http_df = pd.read_csv(f'{_raw_path}filtered_http.csv')\n",
    "\n",
    "# Load labels dataset\n",
    "labels_df = pd.read_csv(f'{_label_path}user_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de4c00e",
   "metadata": {},
   "source": [
    "- Menggabungkan `users_df` dengan `labels_df` untuk mendapatkan label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "610c014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(users_df.shape)\n",
    "# print(labels_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43e142bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_df = users_df.merge(labels_df[['user_id','label']], on='user_id', how='left')\n",
    "# users_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989449e9",
   "metadata": {},
   "source": [
    "- Ambil user menjadi 1000 ID, dengan 998 normal, dan 2 anomali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec585f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_name</th>\n",
       "      <th>user_id</th>\n",
       "      <th>email</th>\n",
       "      <th>role</th>\n",
       "      <th>projects</th>\n",
       "      <th>business_unit</th>\n",
       "      <th>functional_unit</th>\n",
       "      <th>department</th>\n",
       "      <th>team</th>\n",
       "      <th>supervisor</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Murphy Benjamin Gentry</td>\n",
       "      <td>MBG3183</td>\n",
       "      <td>Murphy.Benjamin.Gentry@dtaa.com</td>\n",
       "      <td>ElectricalEngineer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3 - ResearchAndEngineering_Government_Domestic</td>\n",
       "      <td>4 - Engineering</td>\n",
       "      <td>24 - SystemsEngineering</td>\n",
       "      <td>Malcolm Elton Battle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chandler Martin Page</td>\n",
       "      <td>CMP2946</td>\n",
       "      <td>Chandler.Martin.Page@dtaa.com</td>\n",
       "      <td>Salesman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>9 - SalesAndMarketing_Government</td>\n",
       "      <td>2 - Sales</td>\n",
       "      <td>4 - RegionalSales</td>\n",
       "      <td>Tanisha Chiquita Mullins</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kirestin Kylan Carter</td>\n",
       "      <td>KKC2119</td>\n",
       "      <td>Kirestin.Kylan.Carter@dtaa.com</td>\n",
       "      <td>SystemsEngineer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4 - ResearchAndEngineering_Government_Foreign</td>\n",
       "      <td>4 - Engineering</td>\n",
       "      <td>15 - SystemsEngineering</td>\n",
       "      <td>Leo Travis Rojas</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barry Xavier Knowles</td>\n",
       "      <td>BXK2091</td>\n",
       "      <td>Barry.Xavier.Knowles@dtaa.com</td>\n",
       "      <td>ElectricalEngineer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4 - ResearchAndEngineering_Government_Foreign</td>\n",
       "      <td>4 - Engineering</td>\n",
       "      <td>13 - ElectricalEngineering</td>\n",
       "      <td>Barrett Ulysses Shepherd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tatum Anastasia Fletcher</td>\n",
       "      <td>TAF0640</td>\n",
       "      <td>Tatum.Anastasia.Fletcher@dtaa.com</td>\n",
       "      <td>WebDeveloper</td>\n",
       "      <td>Project 199</td>\n",
       "      <td>1</td>\n",
       "      <td>3 - ResearchAndEngineering_Government_Domestic</td>\n",
       "      <td>3 - SoftwareManagement</td>\n",
       "      <td>4 - WebSoftware</td>\n",
       "      <td>Winifred Kelsie Garza</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              employee_name  user_id                              email  \\\n",
       "0    Murphy Benjamin Gentry  MBG3183    Murphy.Benjamin.Gentry@dtaa.com   \n",
       "1      Chandler Martin Page  CMP2946      Chandler.Martin.Page@dtaa.com   \n",
       "2     Kirestin Kylan Carter  KKC2119     Kirestin.Kylan.Carter@dtaa.com   \n",
       "3      Barry Xavier Knowles  BXK2091      Barry.Xavier.Knowles@dtaa.com   \n",
       "4  Tatum Anastasia Fletcher  TAF0640  Tatum.Anastasia.Fletcher@dtaa.com   \n",
       "\n",
       "                 role     projects  business_unit  \\\n",
       "0  ElectricalEngineer          NaN              1   \n",
       "1            Salesman          NaN              1   \n",
       "2     SystemsEngineer          NaN              1   \n",
       "3  ElectricalEngineer          NaN              1   \n",
       "4        WebDeveloper  Project 199              1   \n",
       "\n",
       "                                  functional_unit              department  \\\n",
       "0  3 - ResearchAndEngineering_Government_Domestic         4 - Engineering   \n",
       "1                9 - SalesAndMarketing_Government               2 - Sales   \n",
       "2   4 - ResearchAndEngineering_Government_Foreign         4 - Engineering   \n",
       "3   4 - ResearchAndEngineering_Government_Foreign         4 - Engineering   \n",
       "4  3 - ResearchAndEngineering_Government_Domestic  3 - SoftwareManagement   \n",
       "\n",
       "                         team                supervisor  label  \n",
       "0     24 - SystemsEngineering      Malcolm Elton Battle      1  \n",
       "1           4 - RegionalSales  Tanisha Chiquita Mullins      1  \n",
       "2     15 - SystemsEngineering          Leo Travis Rojas      0  \n",
       "3  13 - ElectricalEngineering  Barrett Ulysses Shepherd      0  \n",
       "4             4 - WebSoftware     Winifred Kelsie Garza      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insider_users = users_df[users_df['label'] == 1]\n",
    "# normal_users = users_df[users_df['label'] == 0].sample(n=998, random_state=42)\n",
    "normal_users = users_df[users_df['label'] == 0]\n",
    "filtered_users = pd.concat([insider_users, normal_users]).reset_index(drop=True)\n",
    "filtered_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516bcbf",
   "metadata": {},
   "source": [
    "- Ubah nama fitur `user` menjadi `user_id` agar konsisten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1242285a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>pc</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/02/2010 02:19:18</td>\n",
       "      <td>DNS1758</td>\n",
       "      <td>PC-0414</td>\n",
       "      <td>Logon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/02/2010 02:31:12</td>\n",
       "      <td>DNS1758</td>\n",
       "      <td>PC-0414</td>\n",
       "      <td>Logoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/02/2010 02:34:02</td>\n",
       "      <td>DNS1758</td>\n",
       "      <td>PC-5313</td>\n",
       "      <td>Logon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/02/2010 02:53:30</td>\n",
       "      <td>DNS1758</td>\n",
       "      <td>PC-5313</td>\n",
       "      <td>Logoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/02/2010 04:07:31</td>\n",
       "      <td>DNS1758</td>\n",
       "      <td>PC-0012</td>\n",
       "      <td>Logon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  user_id       pc activity\n",
       "0  01/02/2010 02:19:18  DNS1758  PC-0414    Logon\n",
       "1  01/02/2010 02:31:12  DNS1758  PC-0414   Logoff\n",
       "2  01/02/2010 02:34:02  DNS1758  PC-5313    Logon\n",
       "3  01/02/2010 02:53:30  DNS1758  PC-5313   Logoff\n",
       "4  01/02/2010 04:07:31  DNS1758  PC-0012    Logon"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logon dataframe\n",
    "logon_df = logon_df.drop(columns={'id'}).rename(columns={'user': 'user_id'})\n",
    "logon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e285d9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>pc</th>\n",
       "      <th>filename</th>\n",
       "      <th>activity</th>\n",
       "      <th>to_removable_media</th>\n",
       "      <th>from_removable_media</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/02/2010 07:19:41</td>\n",
       "      <td>SDH2394</td>\n",
       "      <td>PC-5849</td>\n",
       "      <td>R:\\60WBQE7S.doc</td>\n",
       "      <td>File Open</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>D0-CF-11-E0-A1-B1-1A-E1 Ernesztin's brother, L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/02/2010 07:21:30</td>\n",
       "      <td>SDH2394</td>\n",
       "      <td>PC-5849</td>\n",
       "      <td>R:\\0VGILDW8.pdf</td>\n",
       "      <td>File Write</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>25-50-44-46-2D ---- Bengali As do many other T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/02/2010 07:22:11</td>\n",
       "      <td>SDH2394</td>\n",
       "      <td>PC-5849</td>\n",
       "      <td>R:\\60WBQE7S.doc</td>\n",
       "      <td>File Copy</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>D0-CF-11-E0-A1-B1-1A-E1 Ernesztin's brother, L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/02/2010 07:24:06</td>\n",
       "      <td>SDH2394</td>\n",
       "      <td>PC-5849</td>\n",
       "      <td>R:\\22B5gX4\\H8Y96RRE.doc</td>\n",
       "      <td>File Write</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>D0-CF-11-E0-A1-B1-1A-E1 After the death of his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/02/2010 07:24:45</td>\n",
       "      <td>SDH2394</td>\n",
       "      <td>PC-5849</td>\n",
       "      <td>R:\\SDH2394\\7XRCV2N5.pdf</td>\n",
       "      <td>File Copy</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>25-50-44-46-2D Although he restored some of th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  user_id       pc                 filename    activity  \\\n",
       "0  01/02/2010 07:19:41  SDH2394  PC-5849          R:\\60WBQE7S.doc   File Open   \n",
       "1  01/02/2010 07:21:30  SDH2394  PC-5849          R:\\0VGILDW8.pdf  File Write   \n",
       "2  01/02/2010 07:22:11  SDH2394  PC-5849          R:\\60WBQE7S.doc   File Copy   \n",
       "3  01/02/2010 07:24:06  SDH2394  PC-5849  R:\\22B5gX4\\H8Y96RRE.doc  File Write   \n",
       "4  01/02/2010 07:24:45  SDH2394  PC-5849  R:\\SDH2394\\7XRCV2N5.pdf   File Copy   \n",
       "\n",
       "   to_removable_media  from_removable_media  \\\n",
       "0               False                  True   \n",
       "1                True                 False   \n",
       "2               False                  True   \n",
       "3                True                 False   \n",
       "4                True                 False   \n",
       "\n",
       "                                             content  \n",
       "0  D0-CF-11-E0-A1-B1-1A-E1 Ernesztin's brother, L...  \n",
       "1  25-50-44-46-2D ---- Bengali As do many other T...  \n",
       "2  D0-CF-11-E0-A1-B1-1A-E1 Ernesztin's brother, L...  \n",
       "3  D0-CF-11-E0-A1-B1-1A-E1 After the death of his...  \n",
       "4  25-50-44-46-2D Although he restored some of th...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File dataframe\n",
    "file_df = file_df.drop(columns={'id'}).rename(columns={'user': 'user_id'})\n",
    "file_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86f74f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>pc</th>\n",
       "      <th>file_tree</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/02/2010 07:17:18</td>\n",
       "      <td>SDH2394</td>\n",
       "      <td>PC-5849</td>\n",
       "      <td>R:\\;R:\\22B5gX4;R:\\SDH2394</td>\n",
       "      <td>Connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/02/2010 07:22:42</td>\n",
       "      <td>JKS2444</td>\n",
       "      <td>PC-6961</td>\n",
       "      <td>R:\\;R:\\JKS2444</td>\n",
       "      <td>Connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/02/2010 07:31:42</td>\n",
       "      <td>CBA1023</td>\n",
       "      <td>PC-1570</td>\n",
       "      <td>R:\\;R:\\42gY283;R:\\48rr4y2;R:\\59ntt61;R:\\76xCQG...</td>\n",
       "      <td>Connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/02/2010 07:33:28</td>\n",
       "      <td>GNT0221</td>\n",
       "      <td>PC-6427</td>\n",
       "      <td>R:\\;R:\\GNT0221</td>\n",
       "      <td>Connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/02/2010 07:33:55</td>\n",
       "      <td>JKS2444</td>\n",
       "      <td>PC-6961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Disconnect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  user_id       pc  \\\n",
       "0  01/02/2010 07:17:18  SDH2394  PC-5849   \n",
       "1  01/02/2010 07:22:42  JKS2444  PC-6961   \n",
       "2  01/02/2010 07:31:42  CBA1023  PC-1570   \n",
       "3  01/02/2010 07:33:28  GNT0221  PC-6427   \n",
       "4  01/02/2010 07:33:55  JKS2444  PC-6961   \n",
       "\n",
       "                                           file_tree    activity  \n",
       "0                          R:\\;R:\\22B5gX4;R:\\SDH2394     Connect  \n",
       "1                                     R:\\;R:\\JKS2444     Connect  \n",
       "2  R:\\;R:\\42gY283;R:\\48rr4y2;R:\\59ntt61;R:\\76xCQG...     Connect  \n",
       "3                                     R:\\;R:\\GNT0221     Connect  \n",
       "4                                                NaN  Disconnect  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device dataframe\n",
    "device_df = device_df.drop(columns={'id'}).rename(columns={'user': 'user_id'})\n",
    "device_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d791cae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>url</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/02/2010 06:38:15</td>\n",
       "      <td>LSN1672</td>\n",
       "      <td>foodnetwork.com</td>\n",
       "      <td>Visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/02/2010 06:39:44</td>\n",
       "      <td>LSN1672</td>\n",
       "      <td>wordpress.com</td>\n",
       "      <td>Visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/02/2010 06:40:11</td>\n",
       "      <td>LSN1672</td>\n",
       "      <td>examiner.com</td>\n",
       "      <td>Visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/02/2010 06:47:43</td>\n",
       "      <td>LSN1672</td>\n",
       "      <td>apple.com</td>\n",
       "      <td>Download</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/02/2010 06:48:22</td>\n",
       "      <td>LSN1672</td>\n",
       "      <td>latimes.com</td>\n",
       "      <td>Visit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  user_id              url  activity\n",
       "0  01/02/2010 06:38:15  LSN1672  foodnetwork.com     Visit\n",
       "1  01/02/2010 06:39:44  LSN1672    wordpress.com     Visit\n",
       "2  01/02/2010 06:40:11  LSN1672     examiner.com     Visit\n",
       "3  01/02/2010 06:47:43  LSN1672        apple.com  Download\n",
       "4  01/02/2010 06:48:22  LSN1672      latimes.com     Visit"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HTTP dataframe\n",
    "# http_df = http_df.drop(columns={'id}).rename(columns={'user': 'user_id'})\n",
    "http_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3aa2e",
   "metadata": {},
   "source": [
    "- Filter semua dataset berdasarkan user yang dipilih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94868f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = set(filtered_users['user_id'])\n",
    "logon_df = logon_df[logon_df['user_id'].isin(user_ids)]\n",
    "file_df = file_df[file_df['user_id'].isin(user_ids)]\n",
    "device_df = device_df[device_df['user_id'].isin(user_ids)]\n",
    "http_df = http_df[http_df['user_id'].isin(user_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "715bb761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 1000 users\n",
      "Insider users: 2, Normal users: 998\n",
      "Label distribution: [998   2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Filtered to {len(filtered_users)} users\")\n",
    "print(f\"Insider users: {len(insider_users)}, Normal users: {len(normal_users)}\")\n",
    "print(f\"Label distribution: {np.bincount(filtered_users['label'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f893c85",
   "metadata": {},
   "source": [
    "## Nodes & Edges Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d588e635",
   "metadata": {},
   "source": [
    "### Ekstrak User-features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d00e4",
   "metadata": {},
   "source": [
    "`step 3`: Mengekstrak fitur user yang dibutuhkan. </br>\n",
    "Struktur node user </br>\n",
    "Sumber Data: users.csv + agregasi dari semua log files </br>\n",
    "Fitur Node:\n",
    "\n",
    "- role_encoded: Encoding jabatan berdasarkan users.csv\n",
    "- department_encoded: Encoding departemen berdasarkan users.csv\n",
    "- total_logon_events: Agregasi dari logon.csv\n",
    "- total_file_events: Agregasi dari file.csv\n",
    "- total_device_events: Agregasi dari device.csv\n",
    "- total_http_events: Agregasi dari http.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55fcb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung statistik per user\n",
    "user_logon_stats = logon_df['user_id'].value_counts().to_frame('total_logon_events').reset_index()\n",
    "user_file_stats = file_df['user_id'].value_counts().to_frame('total_file_events').reset_index()\n",
    "user_device_stats = device_df['user_id'].value_counts().to_frame('total_device_events').reset_index()\n",
    "user_http_stats = http_df['user_id'].value_counts().to_frame('total_http_events').reset_index()\n",
    "\n",
    "# Gabungkan semua statistik User\n",
    "user_features_df = filtered_users.merge(user_logon_stats, on='user_id', how='left')\n",
    "user_features_df = user_features_df.merge(user_file_stats, on='user_id', how='left')\n",
    "user_features_df = user_features_df.merge(user_device_stats, on='user_id', how='left')\n",
    "user_features_df = user_features_df.merge(user_http_stats, on='user_id', how='left').fillna(0)\n",
    "\n",
    "user_features = {\n",
    "    'user_id': user_features_df['user_id'].values,\n",
    "    'role': user_features_df['role'].values,\n",
    "    'department': user_features_df['department'].values,\n",
    "    'labels': user_features_df['label'].values,\n",
    "    'total_logon_events': user_features_df['total_logon_events'].values,\n",
    "    'total_file_events': user_features_df['total_file_events'].values,\n",
    "    'total_device_events': user_features_df['total_device_events'].values,\n",
    "    'total_http_events': user_features_df['total_http_events'].values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "776447f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Role encoding\n",
    "role_encoder = LabelEncoder()\n",
    "user_features['role_encoded'] = role_encoder.fit_transform(user_features['role'].astype(str))\n",
    "\n",
    "# Department encoding  \n",
    "dept_encoder = LabelEncoder()\n",
    "user_features['department_encoded'] = dept_encoder.fit_transform(user_features['department'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a509a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User features extracted for 1000 users\n",
      "Features: ['user_id', 'role', 'department', 'labels', 'total_logon_events', 'total_file_events', 'total_device_events', 'total_http_events', 'role_encoded', 'department_encoded']\n"
     ]
    }
   ],
   "source": [
    "print(f\"User features extracted for {len(user_features['user_id'])} users\")\n",
    "print(f\"Features: {list(user_features.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b4cb8",
   "metadata": {},
   "source": [
    "### Ekstrak PC-features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5b404",
   "metadata": {},
   "source": [
    "`step 4`: Ekstrak fitur untuk node PC berdasarkan agregasi aktivitas dari logon, file, dan device logs. </br>\n",
    "Sumber Data: Agregasi dari logon.csv, file.csv, device.csv </br>\n",
    "Fitur Node:\n",
    "- unique_users_count: Jumlah user unik yang menggunakan PC ini\n",
    "- avg_daily_logons: Rata-rata logon per hari ke PC ini\n",
    "- total_file_operations: Total operasi file di PC ini\n",
    "- total_device_connections: Total koneksi device di PC ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4946fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstrak fitur untuk node PC\n",
    "pc_logon_stats = logon_df.groupby('pc').agg({'user_id': 'nunique', 'date': 'count'}).reset_index()\n",
    "pc_logon_stats.columns = ['pc', 'unique_users_count', 'total_logons']\n",
    "\n",
    "pc_file_stats = file_df.groupby('pc').agg({'user_id': 'count'}).reset_index()\n",
    "pc_file_stats.columns = ['pc', 'total_file_operations']\n",
    "\n",
    "pc_device_stats = device_df.groupby('pc').agg({'user_id': 'count'}).reset_index()\n",
    "pc_device_stats.columns = ['pc', 'total_device_connections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e30da61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan semua statistik PC\n",
    "pc_features_df = pc_logon_stats.merge(pc_file_stats, on='pc', how='left')\n",
    "pc_features_df = pc_features_df.merge(pc_device_stats, on='pc', how='left').fillna(0)\n",
    "pc_features_df['avg_daily_logons'] = pc_features_df['total_logons'] / 365\n",
    "\n",
    "pc_features = {\n",
    "    'pc': pc_features_df['pc'].values,\n",
    "    'unique_users_count': pc_features_df['unique_users_count'].values,\n",
    "    'avg_daily_logons': pc_features_df['avg_daily_logons'].values,\n",
    "    'total_file_operations': pc_features_df['total_file_operations'].values,\n",
    "    'total_device_connections': pc_features_df['total_device_connections'].values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7800789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC features extracted for 4298 PCs\n",
      "Features: ['pc', 'unique_users_count', 'avg_daily_logons', 'total_file_operations', 'total_device_connections']\n"
     ]
    }
   ],
   "source": [
    "print(f\"PC features extracted for {len(pc_features['pc'])} PCs\")\n",
    "print(f\"Features: {list(pc_features.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72df23c9",
   "metadata": {},
   "source": [
    "### Ekstrak URL-features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774fb14",
   "metadata": {},
   "source": [
    "`step 5`: Kategorisasi domain URL berdasarkan jenis website dan ekstrak fitur untuk node URL berdasarkan pola akses. </br>\n",
    "Sumber Data: http.csv </br>\n",
    "Fitur Node: </br>\n",
    "- domain_category: Kategori berdasarkan domain (work_related, cloud_storage, job_search, social_media, other)\n",
    "- total_visits: Total kunjungan ke URL ini\n",
    "- unique_visitors: Jumlah user unik yang mengakses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8984ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kategorisasi domain URL berdasarkan jenis\n",
    "def categorize_url_domain(url):\n",
    "    if pd.isna(url) or url is None:\n",
    "        return 'unknown'\n",
    "    url_lower = str(url).lower()\n",
    "    if any(x in url_lower for x in ['dropbox', 'inbox', 'redbox', 'soundcloud']):\n",
    "        return 'cloud_storage'\n",
    "    elif any(x in url_lower for x in ['careerbuilder', 'foodnetwork', 'howstuffworks', 'indeed', 'job-hunt.org', 'jobhuntersbible', 'linkedin', 'monster', 'networkedblogs', 'networksolutions']):\n",
    "        return 'job_search'\n",
    "    elif any(x in url_lower for x in ['facebook', 'livingsocial', 'reddit', 'twitter']):\n",
    "        return 'social_media'\n",
    "    elif any(x in url_lower for x in ['.gov', '.edu', 'officedepot']):\n",
    "        return 'work_related'\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "862e7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstrak fitur untuk node URL\n",
    "url_stats = http_df.groupby('url').agg({'user_id': ['count', 'nunique']}).reset_index()\n",
    "url_stats.columns = ['url', 'total_visits', 'unique_visitors']\n",
    "url_stats['domain_category'] = url_stats['url'].apply(categorize_url_domain)\n",
    "\n",
    "url_features = {\n",
    "    'url': url_stats['url'].values,\n",
    "    'domain_category': url_stats['domain_category'].values,\n",
    "    'total_visits': url_stats['total_visits'].values,\n",
    "    'unique_visitors': url_stats['unique_visitors'].values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "941df571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL features extracted for 476 URLs\n",
      "URL categories: ['cloud_storage' 'job_search' 'other' 'social_media' 'work_related']\n",
      "Features: ['url', 'domain_category', 'total_visits', 'unique_visitors']\n"
     ]
    }
   ],
   "source": [
    "print(f\"URL features extracted for {len(url_features['url'])} URLs\")\n",
    "print(f\"URL categories: {np.unique(url_features['domain_category'])}\")\n",
    "print(f\"Features: {list(url_features.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f9e910",
   "metadata": {},
   "source": [
    "### Ekstrak User-PC Edge Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3b650",
   "metadata": {},
   "source": [
    "`step 6`: Ekstrak fitur untuk edge User→PC (user mengakses pc) berdasarkan pola logon, file operations, dan device usage. </br>\n",
    "Sumber Data: logon.csv, file.csv, device.csv </br>\n",
    "Fitur Edge:\n",
    "- logon_count: Jumlah event Logon\n",
    "- logoff_count: Jumlah event Logoff\n",
    "- file_open_count: File operations dengan activity=\"Open\"\n",
    "- file_write_count: File operations dengan activity=\"Write\"\n",
    "- file_copy_count: File operations dengan activity=\"Copy\"\n",
    "- file_delete_count: File operations dengan activity=\"Delete\"\n",
    "- device_connect_count: Device events dengan activity=\"Connect\"\n",
    "- device_disconnect_count: Device events dengan activity=\"Disconnect\"\n",
    "- after_hours_logon: Login di luar jam kerja (sebelum 07:00 atau setelah 17:00)\n",
    "- weekend_logon: Login di akhir pekan (Sabtu-Minggu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6e53ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstrak fitur untuk edge User→PC\n",
    "logon_df['datetime'] = pd.to_datetime(logon_df['date'])\n",
    "logon_df['hour'] = logon_df['datetime'].dt.hour\n",
    "logon_df['dayofweek'] = logon_df['datetime'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b1cc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitur logon (login/logout, jam kerja, weekend)\n",
    "logon_features = logon_df.groupby(['user_id', 'pc']).agg({\n",
    "    'activity': lambda x: (x == 'Logon').sum(),\n",
    "    'hour': lambda x: sum((x < 7) | (x > 17)),\n",
    "    'dayofweek': lambda x: sum(x >= 5)\n",
    "}).reset_index()\n",
    "logon_features.columns = ['user_id', 'pc', 'logon_count', 'after_hours_logon', 'weekend_logon']\n",
    "\n",
    "# Hitung logoff count\n",
    "logoff_count = logon_df[logon_df['activity'] == 'Logoff'].groupby(['user_id', 'pc']).size().reset_index()\n",
    "logoff_count.columns = ['user_id', 'pc', 'logoff_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2dec5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitur file operations (open, write, copy, delete)\n",
    "file_activities = ['File Open', 'File Write', 'File Copy', 'File Delete']\n",
    "result = []\n",
    "for (user_id, pc), group in file_df.groupby(['user_id', 'pc']):\n",
    "    counts = [sum(group['activity'] == act) for act in file_activities]\n",
    "    result.append([user_id, pc] + counts)\n",
    "\n",
    "file_features = pd.DataFrame(result, columns=['user_id', 'pc', 'file_open_count', 'file_write_count', 'file_copy_count', 'file_delete_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c96f7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitur device (connect/disconnect USB)\n",
    "device_features = device_df.groupby(['user_id', 'pc'])['activity'].apply(\n",
    "    lambda x: pd.Series([sum(x == 'Connect'), sum(x == 'Disconnect')])\n",
    ").reset_index()\n",
    "device_features.columns = ['user_id', 'pc', 'device_connect_count', 'device_disconnect_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "952f5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan semua fitur edge User-PC\n",
    "user_pc_edges = logon_features.merge(logoff_count, on=['user_id', 'pc'], how='left')\n",
    "user_pc_edges = user_pc_edges.merge(file_features, on=['user_id', 'pc'], how='left')\n",
    "user_pc_edges = user_pc_edges.merge(device_features, on=['user_id', 'pc'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de35211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-PC edges: 44991\n",
      "Features: ['user_id', 'pc', 'logon_count', 'after_hours_logon', 'weekend_logon', 'logoff_count', 'file_open_count', 'file_write_count', 'file_copy_count', 'file_delete_count', 'device_connect_count', 'device_disconnect_count']\n"
     ]
    }
   ],
   "source": [
    "print(f\"User-PC edges: {len(user_pc_edges)}\")\n",
    "print(f\"Features: {list(user_pc_edges.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4944c413",
   "metadata": {},
   "source": [
    "### Ekstrak User-URL Edge Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10337796",
   "metadata": {},
   "source": [
    "`step 7`: Ekstrak fitur untuk edge User→URL (user mengunjungi situs) berdasarkan pola browsing dan akses ke kategori website tertentu. </br>\n",
    "Sumber Data: http.csv </br>\n",
    "Fitur Edge:\n",
    "- visit_frequency: Jumlah kunjungan ke URL\n",
    "- cloud_service_visits: Kunjungan ke layanan cloud (filter berdasarkan domain)\n",
    "- job_site_visits: Kunjungan ke situs lowongan kerja (filter berdasarkan domain)\n",
    "- unique_visit_days: Jumlah hari berbeda mengakses URL\n",
    "- after_hours_browsing: Web browsing di luar jam kerja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf51f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstrak fitur untuk edge User→URL\n",
    "http_df['datetime'] = pd.to_datetime(http_df['date'])\n",
    "http_df['hour'] = http_df['datetime'].dt.hour\n",
    "http_df['date_only'] = http_df['datetime'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc45045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregasi aktivitas browsing per user-url\n",
    "user_url_edges = http_df.groupby(['user_id', 'url']).agg({\n",
    "    'date_only': ['count', 'nunique'],  # count total visits, nunique untuk unique days\n",
    "    'hour': lambda x: sum((x < 7) | (x > 17))\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten multi-level columns\n",
    "user_url_edges.columns = ['user_id', 'url', 'visit_frequency', 'unique_visit_days', 'after_hours_browsing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat boolean indicator untuk cloud storage dan job search sites\n",
    "user_url_edges['is_cloud_storage'] = user_url_edges['url'].apply(\n",
    "    lambda url: 1 if categorize_url_domain(url) == 'cloud_storage' else 0)\n",
    "user_url_edges['is_job_search'] = user_url_edges['url'].apply(\n",
    "    lambda url: 1 if categorize_url_domain(url) == 'job_search' else 0)\n",
    "\n",
    "# Hitung total kunjungan per user ke cloud storage dan job search sites\n",
    "cloud_visits_per_user = http_df[http_df['url'].apply(categorize_url_domain) == 'cloud_storage'].groupby('user_id').size()\n",
    "job_visits_per_user = http_df[http_df['url'].apply(categorize_url_domain) == 'job_search'].groupby('user_id').size()\n",
    "\n",
    "user_url_edges['cloud_service_visits'] = user_url_edges['user_id'].map(cloud_visits_per_user).fillna(0) * user_url_edges['is_cloud_storage']\n",
    "user_url_edges['job_site_visits'] = user_url_edges['user_id'].map(job_visits_per_user).fillna(0) * user_url_edges['is_job_search']\n",
    "\n",
    "# Drop kolom helper\n",
    "user_url_edges = user_url_edges.drop(['is_cloud_storage', 'is_job_search'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a9de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-URL edges: 171361\n",
      "Features: ['user_id', 'url', 'visit_frequency', 'unique_visit_days', 'after_hours_browsing', 'cloud_service_visits', 'job_site_visits']\n"
     ]
    }
   ],
   "source": [
    "print(f\"User-URL edges: {len(user_url_edges)}\")\n",
    "print(f\"Features: {list(user_url_edges.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e57bfc",
   "metadata": {},
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc46bac",
   "metadata": {},
   "source": [
    "`step 8`: Bagi data menjadi training dan validation set dengan stratified sampling untuk menjaga proporsi kelas insider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564108f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagi data menjadi train dan validation set\n",
    "labels = user_features['labels']\n",
    "user_indices = np.arange(len(labels))\n",
    "\n",
    "train_idx, val_idx = train_test_split(user_indices, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "train_mask = np.zeros(len(labels), dtype=bool)\n",
    "val_mask = np.zeros(len(labels), dtype=bool)\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed9284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes - Train: 800, Val: 200\n",
      "Train labels: 2/800\n",
      "Val labels: 0/200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Split sizes - Train: {train_mask.sum()}, Val: {val_mask.sum()}\")\n",
    "print(f\"Train labels: {labels[train_mask].sum()}/{train_mask.sum()}\")\n",
    "print(f\"Val labels: {labels[val_mask].sum()}/{val_mask.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d6c4a6",
   "metadata": {},
   "source": [
    "## Oversampling Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadbf094",
   "metadata": {},
   "source": [
    "`step 9`: Random Oversampling dilakukan untuk mengatasi class imbalance antara insider dan normal users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7007d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling: [798   2]\n",
      "After oversampling: [798 798]\n"
     ]
    }
   ],
   "source": [
    "# Oversampling untuk mengatasi class imbalance\n",
    "train_indices = np.where(train_mask)[0]\n",
    "train_labels = user_features['labels'][train_indices]\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "train_indices_resampled, train_labels_resampled = ros.fit_resample(\n",
    "    train_indices.reshape(-1, 1), train_labels)\n",
    "train_indices_resampled = train_indices_resampled.flatten()\n",
    "\n",
    "print(f\"Before oversampling: {np.bincount(train_labels)}\")\n",
    "print(f\"After oversampling: {np.bincount(train_labels_resampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update train mask setelah oversampling\n",
    "balanced_train_mask = np.zeros(len(user_features['labels']), dtype=bool)\n",
    "unique_indices = np.unique(train_indices_resampled)\n",
    "balanced_train_mask[unique_indices] = True\n",
    "\n",
    "oversampling_info = {\n",
    "    'resampled_indices': train_indices_resampled,\n",
    "    'resampled_labels': train_labels_resampled,\n",
    "    'original_train_mask': train_mask.copy()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e9c00a",
   "metadata": {},
   "source": [
    "## Normalize Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891ba9f3",
   "metadata": {},
   "source": [
    "`step 10`: Encode categorical features dan normalize numerical features menggunakan StandardScaler yang di-fit pada training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ed7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bersihkan data terlebih dahulu - convert semua ke string dan handle missing values\n",
    "user_features['role'] = np.where(pd.isna(user_features['role']), 'Unknown', user_features['role'].astype(str))\n",
    "user_features['department'] = np.where(pd.isna(user_features['department']), 'Unknown', user_features['department'].astype(str))\n",
    "url_features['domain_category'] = np.where(pd.isna(url_features['domain_category']), 'Unknown', url_features['domain_category'].astype(str))\n",
    "\n",
    "# Encode kategorikal features - fit pada seluruh data, bukan hanya training\n",
    "role_encoder = LabelEncoder()\n",
    "dept_encoder = LabelEncoder()\n",
    "url_cat_encoder = LabelEncoder()\n",
    "\n",
    "role_encoder.fit(user_features['role'])\n",
    "dept_encoder.fit(user_features['department'])\n",
    "url_cat_encoder.fit(url_features['domain_category'])\n",
    "\n",
    "user_features['role_encoded'] = role_encoder.transform(user_features['role'])\n",
    "user_features['department_encoded'] = dept_encoder.transform(user_features['department'])\n",
    "url_features['domain_category_encoded'] = url_cat_encoder.transform(url_features['domain_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f048a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisasi fitur numerik User\n",
    "numerical_user_cols = ['total_logon_events',\n",
    "                      'total_file_events', 'total_device_events', 'total_http_events']\n",
    "\n",
    "user_scaler = StandardScaler()\n",
    "train_user_data = np.column_stack([user_features[col][train_indices] for col in numerical_user_cols])\n",
    "user_scaler.fit(train_user_data)\n",
    "\n",
    "all_user_data = np.column_stack([user_features[col] for col in numerical_user_cols])\n",
    "user_features_scaled = user_scaler.transform(all_user_data)\n",
    "user_features_final = np.column_stack([user_features_scaled, user_features['role_encoded'], user_features['department_encoded']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e370e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisasi fitur PC\n",
    "pc_scaler = StandardScaler()\n",
    "pc_data = np.column_stack([pc_features[col] for col in ['unique_users_count', 'avg_daily_logons', 'total_file_operations', 'total_device_connections']])\n",
    "pc_features_scaled = pc_scaler.fit_transform(pc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c70e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisasi fitur URL\n",
    "url_scaler = StandardScaler()\n",
    "url_data = np.column_stack([url_features['total_visits'], url_features['unique_visitors']])\n",
    "url_features_scaled = url_scaler.fit_transform(url_data)\n",
    "url_features_final = np.column_stack([url_features_scaled, url_features['domain_category_encoded']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3adbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature normalization completed\n"
     ]
    }
   ],
   "source": [
    "# Normalisasi fitur edges\n",
    "user_pc_cols = ['logon_count', 'logoff_count', 'file_open_count', 'file_write_count',\n",
    "                'file_copy_count', 'file_delete_count', 'device_connect_count',\n",
    "                'device_disconnect_count', 'after_hours_logon', 'weekend_logon']\n",
    "user_pc_scaler = StandardScaler()\n",
    "user_pc_data = np.column_stack([user_pc_edges[col] for col in user_pc_cols])\n",
    "user_pc_edges_scaled = user_pc_scaler.fit_transform(user_pc_data)\n",
    "\n",
    "user_url_cols = ['visit_frequency', 'cloud_service_visits', 'job_site_visits', 'unique_visit_days', 'after_hours_browsing']\n",
    "user_url_scaler = StandardScaler()\n",
    "user_url_data = np.column_stack([user_url_edges[col] for col in user_url_cols])\n",
    "user_url_edges_scaled = user_url_scaler.fit_transform(user_url_data)\n",
    "\n",
    "print(\"Feature normalization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54393ec3",
   "metadata": {},
   "source": [
    "## Build Edge Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3e82e7",
   "metadata": {},
   "source": [
    "`step 11`: Buat mapping dari ID ke index dan konversi edge list menjadi tensor format untuk PyTorch Geometric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd9605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat mapping dari ID ke index untuk graph\n",
    "user_id_to_idx = {uid: idx for idx, uid in enumerate(user_features['user_id'])}\n",
    "pc_to_idx = {pc: idx for idx, pc in enumerate(pc_features['pc'])}\n",
    "url_to_idx = {url: idx for idx, url in enumerate(url_features['url'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konversi edge list ke tensor format\n",
    "user_pc_edge_index = []\n",
    "for _, row in user_pc_edges.iterrows():\n",
    "    user_idx = user_id_to_idx[row['user_id']]\n",
    "    pc_idx = pc_to_idx[row['pc']]\n",
    "    user_pc_edge_index.append([user_idx, pc_idx])\n",
    "user_pc_edge_index = np.array(user_pc_edge_index)\n",
    "\n",
    "user_url_edge_index = []\n",
    "for _, row in user_url_edges.iterrows():\n",
    "    user_idx = user_id_to_idx[row['user_id']]\n",
    "    url_idx = url_to_idx[row['url']]\n",
    "    user_url_edge_index.append([user_idx, url_idx])\n",
    "user_url_edge_index = np.array(user_url_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bdff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge indices built\n",
      "User-PC edges: 44991\n",
      "User-URL edges: 171361\n"
     ]
    }
   ],
   "source": [
    "print(\"Edge indices built\")\n",
    "print(f\"User-PC edges: {len(user_pc_edge_index)}\")\n",
    "print(f\"User-URL edges: {len(user_url_edge_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5c21e9",
   "metadata": {},
   "source": [
    "## Build Heterogeneous Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec317123",
   "metadata": {},
   "source": [
    "`step 12`: Konstruksi heterogeneous graph menggunakan PyTorch Geometric dengan semua node, edge, dan attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c768ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat heterogeneous graph menggunakan PyTorch Geometric\n",
    "data = HeteroData()\n",
    "\n",
    "# Set node features\n",
    "data['user'].x = torch.tensor(user_features_final, dtype=torch.float)\n",
    "data['pc'].x = torch.tensor(pc_features_scaled, dtype=torch.float)\n",
    "data['url'].x = torch.tensor(url_features_final, dtype=torch.float)\n",
    "\n",
    "# Set labels dan masks\n",
    "data['user'].y = torch.tensor(user_features['labels'], dtype=torch.long)\n",
    "data['user'].train_mask = torch.tensor(balanced_train_mask, dtype=torch.bool)\n",
    "data['user'].val_mask = torch.tensor(val_mask, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set edges dan edge attributes\n",
    "data['user', 'uses', 'pc'].edge_index = torch.tensor(user_pc_edge_index.T, dtype=torch.long)\n",
    "data['user', 'uses', 'pc'].edge_attr = torch.tensor(user_pc_edges_scaled, dtype=torch.float)\n",
    "\n",
    "data['user', 'visits', 'url'].edge_index = torch.tensor(user_url_edge_index.T, dtype=torch.long)\n",
    "data['user', 'visits', 'url'].edge_attr = torch.tensor(user_url_edges_scaled, dtype=torch.float)\n",
    "\n",
    "data.oversampling_info = oversampling_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf20c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Statistics:\n",
      "- Users: 1000 (6 features)\n",
      "- PCs: 4298 (4 features)\n",
      "- URLs: 477 (3 features)\n",
      "- User-PC edges: 44991\n",
      "- User-URL edges: 171361\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph Statistics:\")\n",
    "print(f\"- Users: {data['user'].x.shape[0]} ({data['user'].x.shape[1]} features)\")\n",
    "print(f\"- PCs: {data['pc'].x.shape[0]} ({data['pc'].x.shape[1]} features)\")\n",
    "print(f\"- URLs: {data['url'].x.shape[0]} ({data['url'].x.shape[1]} features)\")\n",
    "print(f\"- User-PC edges: {data['user', 'uses', 'pc'].edge_index.shape[1]}\")\n",
    "print(f\"- User-URL edges: {data['user', 'visits', 'url'].edge_index.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac87ea",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e1e48c",
   "metadata": {},
   "source": [
    "`step 13`: Simpan processed graph, preprocessing objects, dan metadata untuk digunakan dalam training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b2e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data telah disimpan!\n"
     ]
    }
   ],
   "source": [
    "# Simpan graph dan preprocessing objects\n",
    "torch.save(data, '../data/data_graph.pt')\n",
    "\n",
    "scalers = {\n",
    "    'user_scaler': user_scaler,\n",
    "    'pc_scaler': pc_scaler,\n",
    "    'url_scaler': url_scaler,\n",
    "    'user_pc_scaler': user_pc_scaler,\n",
    "    'user_url_scaler': user_url_scaler\n",
    "}\n",
    "\n",
    "encoders = {\n",
    "    'role_encoder': role_encoder,\n",
    "    'dept_encoder': dept_encoder,\n",
    "    'url_cat_encoder': url_cat_encoder\n",
    "}\n",
    "\n",
    "preprocessing_objects = {**scalers, **encoders}\n",
    "with open('../data/data_objects.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_objects, f)\n",
    "\n",
    "metadata = {\n",
    "    'scenario': 'insider_threat_detection',\n",
    "    'num_users': data['user'].x.shape[0],\n",
    "    'num_pcs': data['pc'].x.shape[0],\n",
    "    'num_urls': data['url'].x.shape[0],\n",
    "    'train_size': data['user'].train_mask.sum().item(),\n",
    "    'val_size': data['user'].val_mask.sum().item(),\n",
    "    'insider_count': data['user'].y.sum().item()\n",
    "}\n",
    "\n",
    "with open('../data/logs/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Data telah disimpan!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
